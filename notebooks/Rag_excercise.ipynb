{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0u4mtUqVH7GA",
        "outputId": "e3999ccb-ceb3-45bb-d7cf-4570d6c0a81e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' pip install -qU     langchain==0.0.292     openai==0.27.4     datasets==2.10.1     pinecone-client==2.2.4     tiktoken==0.5.1\\n    '"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\" pip install -qU \\\n",
        "    langchain==0.0.292 \\\n",
        "    openai==0.27.4 \\\n",
        "    datasets==2.10.1 \\\n",
        "    pinecone-client==2.2.4 \\\n",
        "    tiktoken==0.5.1\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ePlUDHK393Ws"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "#os.environ[\"OPENAI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TDOEn4_O6Prl"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/tema/10X/week7/Precision -RAG/Renv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "chat = ChatOpenAI(\n",
        "    openai_api_key = os.environ['OPENAI_API_KEY'],\n",
        "    model='gpt-3.5-turbo'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "p5lauaqp6Prl"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import (\n",
        "    SystemMessage,\n",
        "    HumanMessage,\n",
        "    AIMessage\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
        "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
        "    HumanMessage(content=\"I'd like to understand string theory.\")\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1XcWkvW6Prm",
        "outputId": "e7716568-2bb1-44cf-c0a3-65869d29b079"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "String theory is a theoretical framework in physics that attempts to reconcile quantum mechanics and general relativity. It posits that the fundamental building blocks of the universe are not particles, but rather tiny, vibrating strings. These strings can have different vibrational modes, which manifest as different particles when observed at different energy levels.\n",
            "\n",
            "String theory suggests that there are additional spatial dimensions beyond the three we experience in our everyday lives. These extra dimensions are compactified and not directly observable at our scale. The theory also predicts the existence of multiple universes, known as the multiverse.\n",
            "\n",
            "String theory has not yet been experimentally confirmed, and it remains a subject of active research and debate in the physics community. It is a highly complex and mathematical theory that requires a solid understanding of quantum mechanics and advanced mathematics to fully grasp.\n"
          ]
        }
      ],
      "source": [
        "res = chat(messages)\n",
        "\n",
        "print(res.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JY3Ul5M6epL-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtCPnPYmeqJR"
      },
      "source": [
        "# Automatic Evaluation Data Generation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmAecA0ke406"
      },
      "source": [
        "##Evaluating RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "heGQBVBVILXN"
      },
      "outputs": [],
      "source": [
        "chunk = 'There is suggestive evidence that photosynthetic organisms were present approximately 3.2 to 3.5 billion years ago, in the form of stromatolites, layered structures similar to forms that are produced by some modern cyanobacteria, as well as numerous microfossils that have been interpreted as arising from phototrophs (Des Marais, 2000).'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JSHGoBjidXb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5juYg5Wmidvf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wkeojrYbY0gy"
      },
      "outputs": [],
      "source": [
        "#Example QA generation prompt\n",
        "from typing import Final\n",
        "GOLD_STANDARD_QA_GENERATION_PROMPT: Final[\n",
        "    str\n",
        "] = \"Given the following paragraph, please generate a single question and its corresponding answer.\\n Question start with a 'Q:' and answers start with an 'A:'. The answer should be the next line after the question. Just output a question and answer according to the instruction.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7eywK9qSZ_03"
      },
      "outputs": [],
      "source": [
        "question =  'When did photosynthetic organisms emerge?'\n",
        "\n",
        "answer = 'Photosynthetic organisms emerged between 3.2 and 3.5 billion years ago.'\n",
        "\n",
        "source = 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2949000/'\n",
        "\n",
        "wrong_answer = 'Photosynthetic organisms emerged between 3.2 and 2.4 billion years ago.'\n",
        "\n",
        "wrong_source = 'https://en.wikipedia.org/wiki/History_of_Earth'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TdBlITz1aGH6"
      },
      "outputs": [],
      "source": [
        "# Question passed into the RAG Chatbot\n",
        "question = \"When did photosynthetic organisms emerge?\"\n",
        "\n",
        "# This would actually be populated from calling our RAG system API\n",
        "rag_api_response = {\n",
        "    \"sources\": [\n",
        "        \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2949000/\"\n",
        "    ],\n",
        "    \"chatbot_response\": [\n",
        "        {\n",
        "            \"message\": \"Photosynthetic organisms emerged between 3.2 and 3.5 billion years ago.\",\n",
        "        }\n",
        "    ],\n",
        "    \"cost\": \"0.001662\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGYJoj0WaKWa",
        "outputId": "40731396-6360-4f11-de31-d65cffab8939"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Photosynthetic organisms emerged between 3.2 and 3.5 billion years ago.', ['https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2949000/'], 0.001662]\n"
          ]
        }
      ],
      "source": [
        "rag_source = rag_api_response['sources']\n",
        "rag_answer = rag_api_response['chatbot_response'][0]['message']\n",
        "rag_cost = float(rag_api_response['cost'])\n",
        "\n",
        "faked_rag_response = [rag_answer, rag_source, rag_cost]\n",
        "print(faked_rag_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Gq22jipycvo0"
      },
      "outputs": [],
      "source": [
        "# Metric Prompt for Evaluation\n",
        "ACCURACY_PROMPT: Final[\n",
        "    str\n",
        "] = \"Looking at the gold standard answer \\n'{answer}'\\n and the language model's answer \\n'{faked_rag_response}'\\n, would you state that the language model's answer is completely accurate?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3nlaA0_Qc39M"
      },
      "outputs": [],
      "source": [
        "# Evaluate Answers\n",
        "def evaluate_answers(metric_prompts : list[str]) -> list[str]:\n",
        "    '''\n",
        "    Main function to evaluate answers.\n",
        "    Args:\n",
        "        prompts (list[str]): Prompts to be evaluated\n",
        "    Returns:\n",
        "      (list[str]): A list with prompts evaluation results\n",
        "    '''\n",
        "\n",
        "    evaluations = []\n",
        "    input_tokens = 0\n",
        "    completion_tokens = 0\n",
        "    for prompt in metric_prompts:\n",
        "        text = prompt\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4\",\n",
        "            temperature=0,\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are an LLM evaluation assistant that will evaluate the given information. Evaluate the following results with 'Yes/No' followed by the 'reason' on a newline. 'Yes/No\\nreason'. If you are unsure, please respond with 'I don't know.\\nNo'.\",\n",
        "                },\n",
        "                {\"role\": \"user\", \"content\": text},\n",
        "            ],\n",
        "        )\n",
        "        input_tokens += response.usage.prompt_tokens\n",
        "        completion_tokens += response.usage.completion_tokens\n",
        "        evaluations.append(response.choices[0].message.content.strip())\n",
        "\n",
        "    return evaluations, input_tokens, completion_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8u99xIt8ifo7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtF3WIwDigJm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
