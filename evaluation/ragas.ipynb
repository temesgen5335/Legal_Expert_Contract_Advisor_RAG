{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1757, which is longer than the specified 500\n",
      "Created a chunk of size 1320, which is longer than the specified 500\n",
      "Created a chunk of size 859, which is longer than the specified 500\n",
      "Created a chunk of size 517, which is longer than the specified 500\n",
      "Created a chunk of size 1345, which is longer than the specified 500\n",
      "Created a chunk of size 2420, which is longer than the specified 500\n",
      "Created a chunk of size 851, which is longer than the specified 500\n",
      "Created a chunk of size 1221, which is longer than the specified 500\n",
      "Created a chunk of size 852, which is longer than the specified 500\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load the data\n",
    "loader = TextLoader('../data/10 Academy Cohort B - Weekly Challenge_ Week - 7.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# Chunk the data\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Weaviate\n",
    "import weaviate\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "\n",
    "# Load OpenAI API key from .env file\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# Setup vector database\n",
    "client = weaviate.Client(\n",
    "  embedded_options = EmbeddedOptions()\n",
    ")\n",
    "\n",
    "# Populate vector database\n",
    "vectorstore = Weaviate.from_documents(\n",
    "    client = client,    \n",
    "    documents = chunks,\n",
    "    embedding = OpenAIEmbeddings(),\n",
    "    by_text = False\n",
    ")\n",
    "\n",
    "# Define vectorstore as retriever to enable semantic search\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /home/tema/10X/week7/Precision -RAG/Renv/lib/python3.12/site-packages (1.32.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/tema/10X/week7/Precision -RAG/Renv/lib/python3.12/site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/tema/10X/week7/Precision -RAG/Renv/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/tema/10X/week7/Precision -RAG/Renv/lib/python3.12/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/tema/10X/week7/Precision -RAG/Renv/lib/python3.12/site-packages (from openai) (2.7.3)\n",
      "Requirement already satisfied: sniffio in /home/tema/10X/week7/Precision -RAG/Renv/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/tema/10X/week7/Precision -RAG/Renv/lib/python3.12/site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/tema/10X/week7/Precision -RAG/Renv/lib/python3.12/site-packages (from openai) (4.12.1)\n",
      "Requirement already satisfied: idna>=2.8 in /home/tema/10X/week7/Precision -RAG/Renv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /home/tema/10X/week7/Precision -RAG/Renv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/tema/10X/week7/Precision -RAG/Renv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/tema/10X/week7/Precision -RAG/Renv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/tema/10X/week7/Precision -RAG/Renv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /home/tema/10X/week7/Precision -RAG/Renv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.18.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import openai python package. Please install it with `pip install openai`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/10X/week7/Precision -RAG/Renv/lib/python3.12/site-packages/langchain_community/chat_models/openai.py:304\u001b[0m, in \u001b[0;36mChatOpenAI.validate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m~/10X/week7/Precision -RAG/Renv/lib/python3.12/site-packages/openai/__init__.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m override\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_types\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NOT_GIVEN, NoneType, NotGiven, Transport, ProxiesTypes\n",
      "File \u001b[0;32m~/10X/week7/Precision -RAG/Renv/lib/python3.12/site-packages/openai/types/__init__.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Batch \u001b[38;5;28;01mas\u001b[39;00m Batch\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image \u001b[38;5;28;01mas\u001b[39;00m Image\n",
      "File \u001b[0;32m~/10X/week7/Precision -RAG/Renv/lib/python3.12/site-packages/openai/types/batch.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_error\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchError\n",
      "File \u001b[0;32m~/10X/week7/Precision -RAG/Renv/lib/python3.12/site-packages/openai/_models.py:21\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerics\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfields\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FieldInfo\n",
      "File \u001b[0;32m~/10X/week7/Precision -RAG/Renv/lib/python3.12/site-packages/pydantic/generics.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"The `generics` module is a backport module from V1.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_migration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getattr_migration\n\u001b[1;32m      4\u001b[0m \u001b[38;5;21m__getattr__\u001b[39m \u001b[38;5;241m=\u001b[39m getattr_migration(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[0;32m~/10X/week7/Precision -RAG/Renv/lib/python3.12/site-packages/pydantic/_migration.py:4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Dict\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version_short\n\u001b[1;32m      6\u001b[0m MOVED_IN_V2 \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic.utils:version_info\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic.version:version_info\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic.error_wrappers:ValidationError\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic:ValidationError\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic.generics:GenericModel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic.BaseModel\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     14\u001b[0m }\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'version_short' from 'pydantic.version' (/home/tema/10X/week7/Precision -RAG/Renv/lib/python3.12/site-packages/pydantic/version.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput_parser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StrOutputParser\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Define LLM\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Define prompt template\u001b[39;00m\n\u001b[1;32m     10\u001b[0m template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are an assistant for question-answering tasks. \u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124mUse the following pieces of retrieved context to answer the question. \u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124mIf you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt know the answer, just say that you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt know. \u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124mAnswer:\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n",
      "File \u001b[0;32m~/10X/week7/Precision -RAG/Renv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:183\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    182\u001b[0m     emit_warning()\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/10X/week7/Precision -RAG/Renv/lib/python3.12/site-packages/pydantic/main.py:339\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel_dump\u001b[39m(\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    322\u001b[0m     serialize_as_any: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    323\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m    324\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Usage docs: https://docs.pydantic.dev/2.7/concepts/serialization/#modelmodel_dump\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \n\u001b[1;32m    326\u001b[0m \u001b[38;5;124;03m    Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;124;03m        mode: The mode in which `to_python` should run.\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03m            If mode is 'json', the output will only contain JSON serializable types.\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;124;03m            If mode is 'python', the output may contain non-JSON-serializable Python objects.\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03m        include: A set of fields to include in the output.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03m        exclude: A set of fields to exclude from the output.\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m        context: Additional context to pass to the serializer.\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;124;03m        by_alias: Whether to use the field's alias in the dictionary key if defined.\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m        exclude_unset: Whether to exclude fields that have not been explicitly set.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m        exclude_defaults: Whether to exclude fields that are set to their default value.\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;124;03m        exclude_none: Whether to exclude fields that have a value of `None`.\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m \u001b[38;5;124;03m        round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;124;03m        warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;124;03m            \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;124;03m        serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03m        A dictionary representation of the model.\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_serializer__\u001b[38;5;241m.\u001b[39mto_python(\n\u001b[1;32m    348\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    349\u001b[0m         mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    359\u001b[0m         serialize_as_any\u001b[38;5;241m=\u001b[39mserialize_as_any,\n\u001b[1;32m    360\u001b[0m     )\n",
      "File \u001b[0;32m~/10X/week7/Precision -RAG/Renv/lib/python3.12/site-packages/pydantic/main.py:1102\u001b[0m, in \u001b[0;36mvalidate_model\u001b[0;34m(model, input_data, cls)\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe `models_as_dict` argument is no longer supported; use a model serializer instead.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dumps_kwargs:\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`dumps_kwargs` keyword arguments are no longer supported.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_dump_json(\n\u001b[1;32m   1104\u001b[0m     include\u001b[38;5;241m=\u001b[39minclude,\n\u001b[1;32m   1105\u001b[0m     exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1109\u001b[0m     exclude_none\u001b[38;5;241m=\u001b[39mexclude_none,\n\u001b[1;32m   1110\u001b[0m )\n",
      "File \u001b[0;32m~/10X/week7/Precision -RAG/Renv/lib/python3.12/site-packages/langchain_community/chat_models/openai.py:307\u001b[0m, in \u001b[0;36mChatOpenAI.validate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import openai python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install openai`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    310\u001b[0m     )\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[1;32m    313\u001b[0m     client_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    314\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai_api_key\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morganization\u001b[39m\u001b[38;5;124m\"\u001b[39m: values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai_organization\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m: values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    322\u001b[0m     }\n",
      "\u001b[0;31mImportError\u001b[0m: Could not import openai python package. Please install it with `pip install openai`."
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# Define LLM\n",
    "model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Define prompt template\n",
    "template = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use two sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Setup RAG pipeline\n",
    "rag_chain = (\n",
    "    {\"context\": retriever,  \"question\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | model\n",
    "    | StrOutputParser() \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "questions = [\"What is the business objective PromptlyTech\", \n",
    "             \"What are the key services provided by PromptlyTech?\",\n",
    "             \"What is prompt engineering?\",\n",
    "            ]\n",
    "ground_truths = [[\"PromptlyTech aims to revolutionize how businesses interact with LLMs, making the technology more accessible, efficient, and effective.\"],\n",
    "                [\"PromptlyTech focuses on Automatic Prompt Generation, Automatic Evaluation Data Generation, and Prompt Testing and Ranking services.\"],\n",
    "                [\"Prompt engineering is the craft of designing queries or statements to guide LLMs to produce desired outcomes.\"]]\n",
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "# Inference\n",
    "for query in questions:\n",
    "  answers.append(rag_chain.invoke(query))\n",
    "  contexts.append([docs.page_content for docs in retriever.get_relevant_documents(query)])\n",
    "\n",
    "# To dict\n",
    "data = {\n",
    "    \"question\": questions, \n",
    "    \"answer\": answers, \n",
    "    \"contexts\": contexts, \n",
    "    \"ground_truths\": ground_truths\n",
    "}\n",
    "\n",
    "# Convert dict to dataset\n",
    "dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Renv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
